{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reggy0/disaster_message_classification/blob/main/Project/compare_word_2_vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b76451a9",
      "metadata": {
        "id": "b76451a9"
      },
      "source": [
        "# Compare word2vec and word counting\n",
        "## performing measurement\n",
        "### Disaster Tweet Classification with Disaster Tweets\n",
        "\n",
        "------------------\n",
        "### GOAL\n",
        "- comparing the performance of different word representation\n",
        "    - word2vec vs. word count\n",
        "- Predicting whether a given tweet is about a real disaster or not\n",
        "- if so, predict a `1`. if not, predict a `0`\n",
        "\n",
        "\n",
        "### Reference\n",
        "- [competition main page](https://www.kaggle.com/c/nlp-getting-started)\n",
        "- [word2vec code](https://www.kaggle.com/slatawa/simple-implementation-of-word2vec)\n",
        "- [word2vec-implementation-for-beginner](https://www.kaggle.com/manavkapadnis/nlp-word2vec-implementation-for-beginner)\n",
        "- [word count](https://www.kaggle.com/datarohitingole/disaster-tweet-classification-ridgeclassifiercv)\n",
        "- [comparing the performance of different Machine Learning Algorithm](https://dibyendudeb.com/comparing-machine-learning-algorithms/)\n",
        "- [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fyBXfKHaDLvY"
      },
      "id": "fyBXfKHaDLvY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3c3f54cb",
      "metadata": {
        "id": "3c3f54cb"
      },
      "source": [
        "# 0. Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cec900b4",
      "metadata": {
        "id": "cec900b4",
        "outputId": "1763e7b8-2798-402c-c069-64c7a06ed00a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'xgboost'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-00a37e875d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#! pip3 install xgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#! pip3 install catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
          ]
        }
      ],
      "source": [
        "# for loading and preprocessing the data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "\n",
        "# for training the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import tree, linear_model, neighbors, naive_bayes, ensemble\n",
        "#! pip3 install xgboost\n",
        "from xgboost import XGBClassifier\n",
        "#! pip3 install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# for evaluating classification model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# for data cleaning\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Keras\n",
        "#! pip3 install tensorflow\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "\n",
        "# for word2ve\n",
        "#! pip3 install gensim\n",
        "#! pip3 install konlpy\n",
        "# import gensim\n",
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# Comparing all machine learning algorithms\n",
        "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd52423",
      "metadata": {
        "id": "afd52423"
      },
      "source": [
        "# 2. Data Preprocessing\n",
        "## Contents\n",
        "### 1. Clean the data\n",
        "- dealing with missing values\n",
        "- replace some commonly occuring shorthands\n",
        "- remove any characters other then alphabets\n",
        "- convert all dicitonary to lower case(for consistency)\n",
        "- lemmatize\n",
        "\n",
        "### 2-1. word tokenization and word2vec\n",
        "\n",
        "### 2-2. Convert text to vectors using Counter vectorizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b529c7",
      "metadata": {
        "id": "22b529c7"
      },
      "source": [
        "\n",
        "\n",
        "## Data Description\n",
        "--------------\n",
        "### Files\n",
        "- `train.csv` : the training set\n",
        "- `test.csv` : the test set\n",
        "- `sample_submission.csv` : a sample submission file in the correct format\n",
        "\n",
        "### Columns\n",
        "- `id` : a unique identifier for each tweet\n",
        "- `text` : the text of the tweet\n",
        "- `location` : the location the tweet was sent from \n",
        "- `keyword` : a particular keyword from th tweet\n",
        "- `target` : in train.csv only, this denotes whether a tweet is about a real disaster(1) or not(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ac692c",
      "metadata": {
        "id": "24ac692c"
      },
      "outputs": [],
      "source": [
        "# loading the data set\n",
        "data_path = './TFIDFInput/'\n",
        "train = pd.read_csv(data_path + 'train_added.csv', encoding='CP949')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c05cc34",
      "metadata": {
        "id": "4c05cc34"
      },
      "outputs": [],
      "source": [
        "print('train_shape:', train.shape)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3807c36",
      "metadata": {
        "id": "c3807c36"
      },
      "outputs": [],
      "source": [
        "# split the data <train : test = 8 : 2>\n",
        "train, test = train_test_split(train, test_size = 0.20, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b371c667",
      "metadata": {
        "id": "b371c667"
      },
      "outputs": [],
      "source": [
        "print('train shape:', train.shape)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef81ca13",
      "metadata": {
        "id": "ef81ca13"
      },
      "outputs": [],
      "source": [
        "print('test shape:', test.shape)\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a58ad306",
      "metadata": {
        "id": "a58ad306"
      },
      "source": [
        "## 1. Clean the data\n",
        "#### (1) Dealing with Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079d857d",
      "metadata": {
        "id": "079d857d"
      },
      "outputs": [],
      "source": [
        "all_data = [train,test]\n",
        "for data in all_data:\n",
        "    data.drop([\"location\", \"id\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49eabf7",
      "metadata": {
        "id": "d49eabf7"
      },
      "outputs": [],
      "source": [
        "# data prepocessing with regrex\n",
        "\n",
        "def remove_URL(text): # remove url pattern in text\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "def remove_html(text): # remove html pattern in text\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    return html.sub(r'', text)\n",
        "    #return re.sub(html, '', text)\n",
        "\n",
        "def remove_punct(text): # remove punctuation in text: (;, ', \", :, ., , etc..)\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8002a66d",
      "metadata": {
        "id": "8002a66d"
      },
      "source": [
        "#### (2) Replace some commonly occuring shorthands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6be14e",
      "metadata": {
        "id": "3e6be14e"
      },
      "outputs": [],
      "source": [
        "for data in all_data:\n",
        "  data['text'] = data['text'].apply(lambda x: remove_URL(x))\n",
        "  data['text'] = data['text'].apply(lambda x: remove_html(x))\n",
        "  data['text'] = data['text'].apply(lambda x: remove_punct(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9741394",
      "metadata": {
        "id": "c9741394"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"you'll\", \"you will\", text)\n",
        "    text = re.sub(r\"i'll\", \"i will\", text)\n",
        "    text = re.sub(r\"she'll\", \"she will\", text)\n",
        "    text = re.sub(r\"he'll\", \"he will\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"there's\", \"there is\", text)\n",
        "    text = re.sub(r\"here's\", \"here is\", text)\n",
        "    text = re.sub(r\"who's\", \"who is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dccb8687",
      "metadata": {
        "id": "dccb8687"
      },
      "outputs": [],
      "source": [
        "train['clean_text'] = train['text'].apply(clean_text)\n",
        "test['clean_text'] = test['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8971e69c",
      "metadata": {
        "id": "8971e69c"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "259e76d4",
      "metadata": {
        "id": "259e76d4"
      },
      "source": [
        "#### (3) remove any characters other then alphabets\n",
        "#### (4) convert all dicitonary to lower case(for consistency)\n",
        "#### (5) lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "835a73da",
      "metadata": {
        "id": "835a73da"
      },
      "outputs": [],
      "source": [
        "def massage_text(text):\n",
        "    ## remove anything other then characters and put everything in lowercase\n",
        "    tweet = re.sub(\"[^a-zA-Z]\", ' ', text) # (3)\n",
        "    tweet = tweet.lower() # (4) \n",
        "    tweet = tweet.split()\n",
        "\n",
        "    lem = WordNetLemmatizer() # (5)\n",
        "    tweet = [lem.lemmatize(word) for word in tweet\n",
        "             if word not in set(stopwords.words('english'))]\n",
        "    tweet = ' '.join(tweet)\n",
        "    return tweet\n",
        "    print('--here goes nothing')\n",
        "    print(text)\n",
        "    print(tweet)\n",
        "\n",
        "train['clean_text'] = train['text'].apply(massage_text)\n",
        "test['clean_text'] = test['text'].apply(massage_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54ab7227",
      "metadata": {
        "id": "54ab7227"
      },
      "outputs": [],
      "source": [
        "train.iloc[0:10][['text','clean_text']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce11ddd7",
      "metadata": {
        "id": "ce11ddd7"
      },
      "source": [
        "## 2-1. Word Tokenization and word2vec\n",
        "- tokenize the clean text column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60470791",
      "metadata": {
        "id": "60470791"
      },
      "outputs": [],
      "source": [
        "train['tokens'] = train['clean_text'].apply(lambda x: word_tokenize(x))\n",
        "test['tokens'] = test['clean_text'].apply(lambda x: word_tokenize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f20b23bd",
      "metadata": {
        "id": "f20b23bd"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb3813e",
      "metadata": {
        "id": "dcb3813e"
      },
      "source": [
        "#### convert our data(words) into vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc20414a",
      "metadata": {
        "id": "fc20414a"
      },
      "outputs": [],
      "source": [
        "# #first, create a list corpus which we would be using to train word2vec mappings\n",
        "# def fn_pre_process_data(doc):\n",
        "#     for rec in doc:\n",
        "#         yield gensim.utils.simple_preprocess(rec)\n",
        "\n",
        "# corpus = list(fn_pre_process_data(train['clean_text']))\n",
        "# corpus += list(fn_pre_process_data(test['clean_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4035089",
      "metadata": {
        "id": "b4035089"
      },
      "outputs": [],
      "source": [
        "# #inititate the embedding model, we will come back to the passed arguments later\n",
        "# print('initiated ...')\n",
        "# wv_model = Word2Vec(corpus,vector_size=150,window=3,min_count=2)\n",
        "# #wv_model.build_vocab(corpus)\n",
        "# wv_model.train(corpus,total_examples=len(corpus),epochs=10)\n",
        "# wv_model.save(data_path + 'word2vec.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edebdcc0",
      "metadata": {
        "id": "edebdcc0"
      },
      "outputs": [],
      "source": [
        "# # convert the train and text tokens\n",
        "# def get_word_embeddings(token_list,vector,k=150):\n",
        "#     if len(token_list) < 1:\n",
        "#         return np.zeros(k)\n",
        "#     else:\n",
        "\n",
        "#         vectorized = [vector.wv[word] if word in vector.wv else np.random.rand(k) for word in token_list] \n",
        "    \n",
        "#     sum = np.sum(vectorized,axis=0)\n",
        "#     ## return the average\n",
        "#     return sum/len(vectorized)       \n",
        "\n",
        "# def get_embeddings(tokens,vector):\n",
        "#         embeddings = tokens.apply(lambda x: get_word_embeddings(x, wv_model))\n",
        "#         return list(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f716c5",
      "metadata": {
        "id": "18f716c5"
      },
      "outputs": [],
      "source": [
        "# train_embeddings = get_embeddings(train['tokens'],wv_model)\n",
        "# test_embeddings = get_embeddings(test['tokens'],wv_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5eb865a",
      "metadata": {
        "id": "f5eb865a"
      },
      "outputs": [],
      "source": [
        "# model_path = './model/'\n",
        "# wv_model.save(model_path + 'word2vec_1.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f7dbf1b",
      "metadata": {
        "id": "9f7dbf1b"
      },
      "source": [
        "## 2-2. Convert text to vectors using Counter vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1a64a0",
      "metadata": {
        "id": "aa1a64a0"
      },
      "source": [
        "### What is the Count Vectorizer?\n",
        "- convert a collection of text documents to a matrix of token counts\n",
        "\n",
        "### How to Use\n",
        "```python\n",
        "# python example code\n",
        "corpus = [\"This is the first document\", \"This document is the second document\", \"And this is the thrid one\"]\n",
        "vectorize = CounterVectorize()\n",
        "X = vectorize.fit_transform(corpus)\n",
        "```\n",
        "- vectorizer.get_feature_names_out()\n",
        "> array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'], ...)\n",
        "- X.toarray()\n",
        "> [[0 1 1 1 0 0 1 0 1]  \n",
        " [0 2 0 1 0 1 1 0 1]  \n",
        " [1 0 0 1 1 0 1 1 1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30c8f8a8",
      "metadata": {
        "id": "30c8f8a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer = CountVectorizer(max_features=2000)\n",
        "\n",
        "X = count_vectorizer.fit_transform(train[\"clean_text\"]).toarray()\n",
        "test_tmp = count_vectorizer.transform(test[\"clean_text\"]).toarray()\n",
        "y = train['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b15b45",
      "metadata": {
        "id": "b8b15b45"
      },
      "outputs": [],
      "source": [
        "X_train = X\n",
        "X_test = test_tmp\n",
        "y_train = train['target']\n",
        "y_test = test['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ed9a72",
      "metadata": {
        "id": "61ed9a72"
      },
      "source": [
        "# 3. Model\n",
        "## Contents\n",
        "- train the model\n",
        "    - RidgeClassifierCV\n",
        "    - sgd classifier\n",
        "    - BernoulliNB \n",
        "    - RandomForest\n",
        "\n",
        "## Model Description\n",
        "--------------\n",
        "### Ensemble\n",
        "- Combine the predictions of several base estimators built with a given learning algorithm \n",
        "    - in order to improve generalizability / robustness over a single estimator.\n",
        "- Boosting of Ensemble types\n",
        "\n",
        "### Performance - f1-score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4335452",
      "metadata": {
        "id": "c4335452"
      },
      "source": [
        "## 3-2. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e57971",
      "metadata": {
        "id": "68e57971"
      },
      "outputs": [],
      "source": [
        "MLA = [\n",
        "    #Ensemble Methods\n",
        "    ensemble.RandomForestClassifier(),\n",
        "    \n",
        "    #GLM\n",
        "    linear_model.RidgeClassifierCV(),\n",
        "    linear_model.SGDClassifier(),\n",
        "    \n",
        "    #Navies Bayes\n",
        "    naive_bayes.BernoulliNB()\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd12a147",
      "metadata": {
        "id": "bd12a147"
      },
      "source": [
        "# Comapring all MLA\n",
        "## word2vec\n",
        "- precision\n",
        "- recall\n",
        "- accuracy\n",
        "- f1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e77c84",
      "metadata": {
        "id": "b5e77c84"
      },
      "outputs": [],
      "source": [
        "train_embeddings = np.array(X_train)\n",
        "test_embeddings = np.array(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cad082",
      "metadata": {
        "id": "c1cad082"
      },
      "outputs": [],
      "source": [
        "# Comparing all machine learning algorithms\n",
        "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\n",
        "\n",
        "row_index = 0\n",
        "MLA_columns = []\n",
        "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
        "\n",
        "for alg in MLA:\n",
        "    predicted = alg.fit(train_embeddings, y_train).predict(test_embeddings)\n",
        "    fp, tp, th = roc_curve(y_test, predicted)\n",
        "\n",
        "    MLA_name = alg.__class__.__name__\n",
        "    MLA_compare.loc[row_index,'MLA used'] = MLA_name\n",
        "    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(train_embeddings,y_train), 4)\n",
        "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(test_embeddings, y_test), 4)\n",
        "    \n",
        "    \n",
        "    recall = recall_score(test['target'], predicted)\n",
        "    precision = precision_score(test['target'], predicted)\n",
        "    MLA_compare.loc[row_index, 'Precission'] = precision\n",
        "    MLA_compare.loc[row_index, 'Recall'] = recall\n",
        "    MLA_compare.loc[row_index, 'F1-score'] = round((2*precision*recall)/(precision+recall),4)\n",
        "    MLA_compare.loc[row_index, 'AUC'] = auc(fp, tp)\n",
        "\n",
        "    row_index+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88994c60",
      "metadata": {
        "id": "88994c60"
      },
      "outputs": [],
      "source": [
        "# f1-score 기준 정렬\n",
        "MLA_compare.sort_values(by = ['F1-score'], ascending = False, inplace = True)    \n",
        "MLA_compare"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1961aa",
      "metadata": {
        "id": "cc1961aa"
      },
      "source": [
        "# Comapring all MLA\n",
        "## wordCount\n",
        "- precision\n",
        "- recall\n",
        "- accuracy\n",
        "- f1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3814e7",
      "metadata": {
        "id": "fd3814e7"
      },
      "outputs": [],
      "source": [
        "# Comparing all machine learning algorithms\n",
        "from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\n",
        "\n",
        "row_index = 0\n",
        "MLA_columns = []\n",
        "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
        "\n",
        "for alg in MLA:\n",
        "    predicted = alg.fit(X_train, y_train).predict(X_test)\n",
        "    fp, tp, th = roc_curve(y_test, predicted)\n",
        "\n",
        "    MLA_name = alg.__class__.__name__\n",
        "    MLA_compare.loc[row_index,'MLA used'] = MLA_name\n",
        "    MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train, y_train), 4)\n",
        "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, y_test), 4)\n",
        "    \n",
        "    \n",
        "    recall = recall_score(y_test, predicted)\n",
        "    precision = precision_score(y_test, predicted)\n",
        "    MLA_compare.loc[row_index, 'Precission'] = precision\n",
        "    MLA_compare.loc[row_index, 'Recall'] = recall\n",
        "    MLA_compare.loc[row_index, 'F1-score'] = round((2*precision*recall)/(precision+recall),4)\n",
        "    MLA_compare.loc[row_index, 'AUC'] = auc(fp, tp)\n",
        "\n",
        "    row_index+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1343027",
      "metadata": {
        "id": "e1343027"
      },
      "outputs": [],
      "source": [
        "# f1-score 기준 정렬\n",
        "MLA_compare.sort_values(by = ['F1-score'], ascending = False, inplace = True)    \n",
        "MLA_compare"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_pytorch_p36",
      "language": "python",
      "name": "conda_pytorch_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "compare_word_2_vec.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}